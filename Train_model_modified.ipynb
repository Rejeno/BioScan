{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9029eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import fingerprint_enhancer\n",
    "import random\n",
    "import albumentations as A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cea3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./Public_Dataset\"\n",
    "img_size = (300, 400)\n",
    "batch_size = 16\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b9ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "def display_dataset_info(data_dir):\n",
    "    class_names = os.listdir(data_dir)  # List all class directories\n",
    "    class_count = {class_name: len(os.listdir(os.path.join(data_dir, class_name))) for class_name in class_names}\n",
    "    \n",
    "    # Print the number of images in each class\n",
    "    print(\"Dataset Class Distribution:\")\n",
    "    for class_name, count in class_count.items():\n",
    "        print(f\"{class_name}: {count} images\")\n",
    "    \n",
    "    # Show a sample image from each class\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        sample_image_path = os.path.join(class_path, os.listdir(class_path)[0])  # Get the first image from the class\n",
    "        \n",
    "        # Read and resize the image\n",
    "        img = cv2.imread(sample_image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        img = cv2.resize(img, img_size)\n",
    "        \n",
    "        # Display the image\n",
    "        plt.subplot(2, 4, idx + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(class_name)\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca4ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to display dataset information\n",
    "display_dataset_info(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c2b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def simple_augment(image):\n",
    "    # --- Rotation ---\n",
    "    angle = random.uniform(-10, 10)\n",
    "    h, w = image.shape\n",
    "    M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1)\n",
    "    image = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "    # --- Brightness ---\n",
    "    brightness_factor = random.uniform(0.8, 1.2)\n",
    "    image = np.clip(image * brightness_factor, 0, 255)\n",
    "\n",
    "    # --- Contrast ---\n",
    "    contrast_factor = random.uniform(0.8, 1.2)\n",
    "    mean = np.mean(image)\n",
    "    image = np.clip((image - mean) * contrast_factor + mean, 0, 255)\n",
    "\n",
    "    return image.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_masked_processing(image):\n",
    "    \"\"\"\n",
    "    Apply Gaussian blur and adaptive thresholding to the fingerprint region.\n",
    "    More conservative approach to preserve ridge patterns.\n",
    "    \"\"\"\n",
    "    # Step 1: Create a binary mask for the fingerprint region\n",
    "    # Use a more conservative threshold to preserve more detail\n",
    "    _, binary_mask = cv2.threshold(image, 180, 255, cv2.THRESH_BINARY_INV)\n",
    "    \n",
    "    # Step 2: Apply lighter Gaussian blur to preserve ridge details\n",
    "    blurred_image = cv2.GaussianBlur(image, (3, 3), 0.5)  # Reduced sigma\n",
    "    \n",
    "    # Step 3: Apply adaptive thresholding with adjusted parameters\n",
    "    threshold_image = cv2.adaptiveThreshold(\n",
    "        blurred_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY, 15, 8  # Increased block size and C value\n",
    "    )\n",
    "    \n",
    "    # Step 4: Isolate the fingerprint area (where mask is white)\n",
    "    enhanced_fingerprint = cv2.bitwise_and(threshold_image, threshold_image, mask=binary_mask)\n",
    "    \n",
    "    # Step 5: Keep the original background (where mask is black)\n",
    "    background = cv2.bitwise_and(image, image, mask=cv2.bitwise_not(binary_mask))\n",
    "    \n",
    "    # Step 6: Combine the enhanced fingerprint and the original background\n",
    "    final_image = cv2.add(enhanced_fingerprint, background)\n",
    "    \n",
    "    # Ensure the result has sufficient variation\n",
    "    if np.std(final_image) < 5:\n",
    "        print(\"Warning: Masked processing resulted in low variation, returning original\")\n",
    "        return image\n",
    "    \n",
    "    return final_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ab800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_image(image_path, img_size, augment=False):\n",
    "    \"\"\"\n",
    "    Loads, resizes, preprocesses, optionally augments, and enhances the fingerprint image.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    assert image is not None, f\"Could not read image: {image_path}\"\n",
    "    image = cv2.resize(image, img_size)\n",
    "\n",
    "    # Apply masked processing\n",
    "    image = apply_masked_processing(image)\n",
    "\n",
    "    # Apply manual augmentation\n",
    "    if augment:\n",
    "        image = simple_augment(image)\n",
    "\n",
    "    # Check if image has sufficient variation before enhancement\n",
    "    if np.std(image) < 10:  # Very low variation\n",
    "        print(f\"Warning: Low variation in image {image_path}, skipping enhancement\")\n",
    "        # Skip enhancement for problematic images\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            # Fingerprint enhancement with error handling\n",
    "            enhanced = fingerprint_enhancer.enhance_fingerprint(image)\n",
    "            # Only use enhanced version if it's valid\n",
    "            if enhanced is not None and np.std(enhanced) > 0:\n",
    "                image = enhanced\n",
    "            else:\n",
    "                print(f\"Warning: Enhancement failed for {image_path}, using original\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Enhancement error for {image_path}: {e}\")\n",
    "            # Continue with original processed image\n",
    "\n",
    "    # Normalize and expand dims\n",
    "    image = (image * 255).astype(np.uint8) if image.dtype != np.uint8 else image\n",
    "    image = image / 255.0\n",
    "    image = np.expand_dims(image, axis=-1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ed9a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageGenerator(Sequence):\n",
    "    def __init__(self, data_dir, img_size=(300, 400), batch_size=16, subset='training', validation_split=0.2, shuffle=True):\n",
    "        self.data_dir = data_dir\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.subset = subset\n",
    "        self.validation_split = validation_split\n",
    "        self.shuffle = shuffle\n",
    "        self.class_names = sorted(os.listdir(data_dir))\n",
    "        self.class_indices = {class_name: idx for idx, class_name in enumerate(self.class_names)}\n",
    "        self.filepaths, self.labels = self._load_dataset()\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def _load_dataset(self):\n",
    "        filepaths = []\n",
    "        labels = []\n",
    "        for class_name in self.class_names:\n",
    "            class_path = os.path.join(self.data_dir, class_name)\n",
    "            image_files = os.listdir(class_path)\n",
    "            image_paths = [os.path.join(class_path, f) for f in image_files]\n",
    "            n_total = len(image_paths)\n",
    "            n_val = int(n_total * self.validation_split)\n",
    "            if self.subset == 'training':\n",
    "                selected = image_paths[n_val:]\n",
    "            else:\n",
    "                selected = image_paths[:n_val]\n",
    "            filepaths.extend(selected)\n",
    "            labels.extend([self.class_indices[class_name]] * len(selected))\n",
    "        return filepaths, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filepaths) // self.batch_size\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_paths = self.filepaths[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        images = [preprocess_image(path, self.img_size) for path in batch_paths]\n",
    "        labels_one_hot = to_categorical(batch_labels, num_classes=len(self.class_names))\n",
    "        \n",
    "        return np.array(images), np.array(labels_one_hot)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            combined = list(zip(self.filepaths, self.labels))\n",
    "            random.shuffle(combined)\n",
    "            self.filepaths, self.labels = zip(*combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f932d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate training and validation generators\n",
    "train_gen = CustomImageGenerator(data_dir=data_dir, img_size=img_size, batch_size=batch_size, subset='training', validation_split=0.2)\n",
    "val_gen = CustomImageGenerator(data_dir=data_dir, img_size=img_size, batch_size=batch_size, subset='validation', validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_counts = Counter(train_gen.labels)\n",
    "valid_counts = Counter(val_gen.labels)\n",
    "\n",
    "print(\"Training samples per class:\")\n",
    "for class_idx, count in train_counts.items():\n",
    "    class_name = train_gen.class_names[class_idx]\n",
    "    print(f\"  {class_name}: {count}\")\n",
    "\n",
    "print(\"Validation samples per class:\")\n",
    "for class_idx, count in valid_counts.items():\n",
    "    class_name = val_gen.class_names[class_idx]\n",
    "    print(f\"  {class_name}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425187c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d73aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(generator, num_samples=4):\n",
    "    \"\"\"\n",
    "    Visualize original, processed, and enhanced images from the generator.\n",
    "    \"\"\"\n",
    "    sample_paths = random.sample(generator.filepaths, num_samples)\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "    \n",
    "    for i, img_path in enumerate(sample_paths):\n",
    "        # Load original image\n",
    "        img_orig = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_orig = cv2.resize(img_orig, generator.img_size)\n",
    "        \n",
    "        # Apply masked processing\n",
    "        img_processed = apply_masked_processing(img_orig)\n",
    "        \n",
    "        # Enhance fingerprint\n",
    "        img_enhanced = fingerprint_enhancer.enhance_fingerprint(img_processed)\n",
    "        img_enhanced = (img_enhanced * 255).astype(np.uint8)\n",
    "        \n",
    "        # Plot images\n",
    "        axes[i, 0].imshow(img_orig, cmap='gray')\n",
    "        axes[i, 0].set_title('Original')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(img_processed, cmap='gray')\n",
    "        axes[i, 1].set_title('Masked + Gaussian')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(img_enhanced, cmap='gray')\n",
    "        axes[i, 2].set_title('Enhanced')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples from the training generator\n",
    "visualize_samples(train_gen, num_samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd014bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the class indices mapping\n",
    "print(\"Class Indices Mapping:\", train_gen.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529d6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class Indices Mapping:\", val_gen.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Use train_gen.labels instead of train_gen.classes\n",
    "class_labels = np.array(train_gen.labels)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(class_labels),\n",
    "    y=class_labels\n",
    ")\n",
    "\n",
    "# Optional: Map weights to class indices\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(\"Class weights:\", class_weights_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4872179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "# Build a Convolutional Neural Network (CNN) Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation=\"relu\", input_shape=(img_size[0], img_size[1], 1)),\n",
    "    BatchNormalization(),  # Normalize activations after convolution\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    BatchNormalization(),  # Normalizing dense layer activations\n",
    "    Dropout(0.5),  # Regularization to prevent overfitting\n",
    "\n",
    "    Dense(num_classes, activation=\"softmax\")  # Output layer\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4300cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def focal_loss(gamma=2., alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "        weight = alpha * K.pow(1 - y_pred, gamma)\n",
    "        loss = weight * cross_entropy\n",
    "        return K.mean(K.sum(loss, axis=-1))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=focal_loss(gamma=2., alpha=0.25),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654511c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "# Callback to reduce learning rate when validation loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    factor=0.5,          # Factor by which the learning rate will be reduced\n",
    "    patience=3,          # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-6          # Lower bound on the learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89abf0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=3,          # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best validation loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aede22",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_gen)\n",
    "validation_steps = len(val_gen)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    "    class_weight=class_weights_dict,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa8c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for a Keras Sequence or generator\n",
    "batch = next(iter(train_gen))\n",
    "print(\"Batch type:\", type(batch))\n",
    "print(\"Batch length:\", len(batch))\n",
    "print(\"X batch shape:\", batch[0].shape)\n",
    "print(\"y batch shape:\", batch[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88973736",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_val = next(iter(val_gen))\n",
    "print(\"Validation batch X shape:\", batch_val[0].shape)\n",
    "print(\"Validation batch y shape:\", batch_val[1].shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
